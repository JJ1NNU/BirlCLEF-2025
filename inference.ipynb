{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":426177,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":347415,"modelId":368675}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport json\nimport glob\nimport cupy\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.onnx\nimport timm\nimport lightgbm as lgb\nfrom tqdm.auto import tqdm\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    # Paths\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    \n    # 모델 경로 직접 지정\n    model_base_path = '/kaggle/input/conv_gbm/pytorch/default/1/'\n    feature_extractor_path = '/kaggle/input/conv_gbm/pytorch/default/1/convnext_feature_extractor_20250605_203642.pth'\n    lgbm_models_dir = '/kaggle/input/conv_gbm/pytorch/default/1/lgbm_models_20250605_203642'\n    \n    # Audio parameters\n    FS = 32000  \n    WINDOW_SIZE = 5  \n    \n    # Mel spectrogram parameters\n    N_FFT = 1024\n    HOP_LENGTH = 512\n    N_MELS = 128\n    FMIN = 50\n    FMAX = 14000\n    TARGET_SHAPE = (256, 256)\n    \n    # Model parameters\n    model_name = 'convnextv2_nano.fcmae'\n    in_channels = 1\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Inference parameters\n    batch_size = 32\n    threshold = 0.5\n    \n    # TTA parameters\n    use_tta = True\n    tta_count = 5\n\n    \n    debug = False\n    debug_count = 3\n\ncfg = CFG()\nprint(f\"Using device: {cfg.device}\")\nprint(f\"Feature extractor path: {cfg.feature_extractor_path}\")\nprint(f\"LightGBM models directory: {cfg.lgbm_models_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvNeXtFeatureExtractor(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        # 훈련 시와 완전히 동일한 설정\n        self.backbone = timm.create_model(\n            'convnextv2_nano.fcmae',\n            pretrained=False,  # 훈련 시와 동일\n            in_chans=cfg.in_channels,\n            num_classes=0,\n            drop_rate=0.2,      # 훈련 시 사용한 설정\n            drop_path_rate=0.1  # 훈련 시 사용한 설정\n        )\n        # 훈련 시 사용한 이중 풀링 구조 완전 복원\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.global_pool = nn.AdaptiveMaxPool2d(1)\n\n    def forward(self, x):\n        features = self.backbone(x)\n        if len(features.shape) == 4:\n            avg_pool = self.pooling(features).view(features.size(0), -1)\n            max_pool = self.global_pool(features).view(features.size(0), -1)\n            # 평균과 최대 풀링 결합 (훈련 시와 동일)\n            return torch.cat([avg_pool, max_pool], dim=1)\n        return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def audio2melspec(audio_data, cfg):\n    \"\"\"Convert audio data to mel spectrogram\"\"\"\n    try:\n        if np.isnan(audio_data).any():\n            mean_signal = np.nanmean(audio_data)\n            audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio_data,\n            sr=cfg.FS,\n            n_fft=cfg.N_FFT,\n            hop_length=cfg.HOP_LENGTH,\n            n_mels=cfg.N_MELS,\n            fmin=cfg.FMIN,\n            fmax=cfg.FMAX,\n            power=2.0\n        )\n\n        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n        mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n        \n        return mel_spec_norm\n        \n    except Exception as e:\n        print(f\"Error in mel spectrogram conversion: {e}\")\n        # 오류 시 영 스펙트로그램 반환\n        return np.zeros((cfg.N_MELS, int(cfg.FS * cfg.WINDOW_SIZE / cfg.HOP_LENGTH) + 1), dtype=np.float32)\n\n        \n    except Exception as e:\n        print(f\"Error in mel spectrogram conversion: {e}\")\n        # 오류 시 영 스펙트로그램 반환\n        return np.zeros((cfg.N_MELS, int(cfg.FS * cfg.WINDOW_SIZE / cfg.HOP_LENGTH) + 1), dtype=np.float32)\n\ndef apply_tta_augmentations(spec):\n    \"\"\"Apply Test-Time Augmentation to spectrogram\"\"\"\n    augmented_specs = []\n    \n    try:\n        # 1. Original (no augmentation)\n        augmented_specs.append(spec.copy())\n        \n        # 2. Time shift augmentations\n        for shift in [-2, 2]:\n            shifted = np.roll(spec, shift, axis=1)\n            augmented_specs.append(shifted)\n        \n        # 3. Frequency shift augmentations\n        for shift in [-1, 1]:\n            shifted = np.roll(spec, shift, axis=0)\n            augmented_specs.append(shifted)\n        \n        # 4. Brightness/contrast augmentations\n        for gain in [0.9, 1.1]:\n            adjusted = np.clip(spec * gain, 0, 1)\n            augmented_specs.append(adjusted)\n        \n        # 5. Gaussian noise augmentation\n        noise_spec = spec + np.random.normal(0, 0.02, spec.shape)\n        noise_spec = np.clip(noise_spec, 0, 1)\n        augmented_specs.append(noise_spec)\n        \n        # 6. Horizontal flip (time reversal)\n        flipped = np.fliplr(spec)\n        augmented_specs.append(flipped)\n        \n    except Exception as e:\n        print(f\"Error in TTA augmentation: {e}\")\n        # 오류 시 원본만 반환\n        augmented_specs = [spec]\n    \n    return augmented_specs\n\n\ndef process_audio_segment(audio_data, cfg):\n    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n    try:\n        if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n            audio_data = np.pad(audio_data, \n                              (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n                              mode='constant')\n        \n        mel_spec = audio2melspec(audio_data, cfg)\n        \n        # Resize if needed\n        if mel_spec.shape != cfg.TARGET_SHAPE:\n            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n            \n        return mel_spec.astype(np.float32)\n        \n    except Exception as e:\n        print(f\"Error processing audio segment: {e}\")\n        # 오류 시 영 스펙트로그램 반환\n        return np.zeros(cfg.TARGET_SHAPE, dtype=np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_models(cfg, num_classes):\n    \"\"\"Load all models - 직접 경로 지정 방식으로 수정\"\"\"\n    models = {}\n    \n    try:\n        # 모델 파일 존재 확인\n        if not os.path.exists(cfg.feature_extractor_path):\n            print(f\"Error: Feature extractor not found at {cfg.feature_extractor_path}\")\n            return {}\n        \n        if not os.path.exists(cfg.lgbm_models_dir):\n            print(f\"Error: LightGBM models directory not found at {cfg.lgbm_models_dir}\")\n            return {}\n        \n        print(f\"Loading PyTorch feature extractor from {cfg.feature_extractor_path}\")\n        \n        # PyTorch 특징 추출기 로드\n        feature_model = ConvNeXtFeatureExtractor(cfg)\n        \n        # 체크포인트 로드 및 검증\n        print(f\"Loading checkpoint...\")\n        checkpoint = torch.load(cfg.feature_extractor_path, map_location=torch.device('cpu'))\n        print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n        \n        # 모델 상태 로드\n        if 'model_state_dict' in checkpoint:\n            feature_model.load_state_dict(checkpoint['model_state_dict'])\n            print(\"Loaded model_state_dict successfully\")\n        else:\n            print(\"Warning: model_state_dict not found in checkpoint, trying direct load...\")\n            feature_model.load_state_dict(checkpoint)\n        \n        feature_model = feature_model.to(cfg.device)\n        feature_model.eval()\n        \n        # 특성 추출기 테스트\n        print(\"Testing feature extractor...\")\n        test_input = torch.randn(1, 1, cfg.TARGET_SHAPE[0], cfg.TARGET_SHAPE[1]).to(cfg.device)\n        with torch.no_grad():\n            test_features = feature_model(test_input)\n            print(f\"Feature extractor output shape: {test_features.shape}\")\n        \n        # Load LightGBM models\n        print(f\"Loading LightGBM models from {cfg.lgbm_models_dir}\")\n        \n        # 실제 존재하는 모델 파일들 확인\n        existing_models = glob.glob(os.path.join(cfg.lgbm_models_dir, 'lgbm_model_class_*.txt'))\n        print(f\"Found {len(existing_models)} LightGBM model files\")\n        \n        lgbm_models = []\n        loaded_count = 0\n        \n        for i in range(num_classes):\n            model_path = os.path.join(cfg.lgbm_models_dir, f'lgbm_model_class_{i}.txt')\n            if os.path.exists(model_path):\n                try:\n                    model = lgb.Booster(model_file=model_path)\n                    lgbm_models.append(model)\n                    loaded_count += 1\n                    if i < 5:  # 처음 5개만 로그 출력\n                        print(f\"✓ Loaded LightGBM model {i}\")\n                except Exception as e:\n                    print(f\"✗ Error loading LightGBM model {i}: {e}\")\n                    lgbm_models.append(None)\n            else:\n                if i < 5:  # 처음 5개만 로그 출력\n                    print(f\"✗ Model file not found: {model_path}\")\n                lgbm_models.append(None)\n        \n        # 유효한 모델 수 확인\n        if loaded_count == 0:\n            print(\"Error: No valid LightGBM models found!\")\n            return {}\n        \n        models['feature_extractor'] = feature_model\n        models['lgbm_models'] = lgbm_models\n        models['use_openvino'] = False  # PyTorch 사용\n        \n        print(f\"✓ Successfully loaded feature extractor and {loaded_count}/{num_classes} LightGBM models\")\n        \n    except Exception as e:\n        print(f\"Error loading models: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {}\n    \n    return models\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_on_spectrogram_with_tta(audio_path, models, cfg, species_ids):\n    \"\"\"Process a single audio file and predict species presence with TTA\"\"\"\n    predictions = []\n    row_ids = []\n    soundscape_id = Path(audio_path).stem\n    \n    feature_extractor = models['feature_extractor']\n    lgbm_models = models['lgbm_models']\n    \n    try:\n        print(f\"Processing {soundscape_id}\")\n        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n        \n        # 샘플 코드와 동일하게 모든 세그먼트 처리\n        segment_length = cfg.FS * cfg.WINDOW_SIZE\n        \n        segment_idx = 0\n        for start_sample in range(0, len(audio_data), segment_length):\n            end_sample = start_sample + segment_length\n            segment_audio = audio_data[start_sample:end_sample]\n            \n            # 5초 미만이면 패딩\n            if len(segment_audio) < segment_length:\n                segment_audio = np.pad(segment_audio, \n                                     (0, segment_length - len(segment_audio)), \n                                     mode='constant')\n            \n            # Row ID 생성 - 샘플 코드와 동일한 방식\n            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n            row_id = f\"{soundscape_id}_{end_time_sec}\"\n            row_ids.append(row_id)\n\n            # Process segment\n            mel_spec = process_audio_segment(segment_audio, cfg)\n            \n            if cfg.use_tta:\n                # Apply TTA\n                augmented_specs = apply_tta_augmentations(mel_spec)\n                \n                # Limit number of augmentations if specified\n                if cfg.tta_count > 0:\n                    augmented_specs = augmented_specs[:cfg.tta_count]\n                \n                segment_predictions = []\n                \n                for aug_spec in augmented_specs:\n                    # Convert to tensor\n                    spec_tensor = torch.tensor(aug_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n                    spec_tensor = spec_tensor.to(cfg.device)\n                    \n                    # Extract features\n                    with torch.no_grad():\n                        features = feature_extractor(spec_tensor)\n                        features = features.cpu().numpy()\n                    \n                    # LightGBM predictions\n                    aug_preds = np.zeros(len(species_ids))\n                    for i, lgb_model in enumerate(lgbm_models):\n                        if lgb_model is not None:  # None 모델 체크\n                            pred = lgb_model.predict(features)\n                            aug_preds[i] = pred[0]\n                        else:\n                            aug_preds[i] = 0.0  # None 모델은 0으로 처리\n                    \n                    segment_predictions.append(aug_preds)\n                    \n                    # 메모리 정리\n                    del spec_tensor\n                    torch.cuda.empty_cache()\n                \n                # Average TTA predictions\n                final_preds = np.mean(segment_predictions, axis=0)\n                \n            else:\n                # No TTA - single prediction\n                spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n                spec_tensor = spec_tensor.to(cfg.device)\n                \n                # Extract features\n                with torch.no_grad():\n                    features = feature_extractor(spec_tensor)\n                    features = features.cpu().numpy()\n                \n                # LightGBM predictions\n                final_preds = np.zeros(len(species_ids))\n                for i, lgb_model in enumerate(lgbm_models):\n                    if lgb_model is not None:  # None 모델 체크\n                        pred = lgb_model.predict(features)\n                        final_preds[i] = pred[0]\n                    else:\n                        final_preds[i] = 0.0  # None 모델은 0으로 처리\n                \n                # 메모리 정리\n                del spec_tensor\n                torch.cuda.empty_cache()\n            \n            # Convert to probabilities using sigmoid\n            final_preds = 1 / (1 + np.exp(-final_preds))\n            predictions.append(final_preds)\n            \n            segment_idx += 1\n            \n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    return row_ids, predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes with TTA\"\"\"\n    try:\n        test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n\n        \n        if cfg.debug:\n            print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n            test_files = test_files[:cfg.debug_count]\n        \n        print(f\"Found {len(test_files)} test soundscapes\")\n        \n        if cfg.use_tta:\n            print(f\"TTA enabled with {cfg.tta_count} augmentations per sample\")\n        else:\n            print(\"TTA disabled - using single prediction per sample\")\n\n        all_row_ids = []\n        all_predictions = []\n\n        for audio_path in tqdm(test_files):\n            row_ids, predictions = predict_on_spectrogram_with_tta(str(audio_path), models, cfg, species_ids)\n            all_row_ids.extend(row_ids)\n            all_predictions.extend(predictions)\n        \n        return all_row_ids, all_predictions\n        \n    except Exception as e:\n        print(f\"Error in inference: {e}\")\n        import traceback\n        traceback.print_exc()\n        return [], []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe with improved compatibility\"\"\"\n    try:\n        print(\"Creating submission dataframe...\")\n\n        # 기본 제출 구조 생성\n        submission_dict = {'row_id': row_ids}\n        for i, species in enumerate(species_ids):\n            submission_dict[species] = [pred[i] for pred in predictions]\n\n        submission_df = pd.DataFrame(submission_dict)\n        \n        # 샘플 제출 파일과 정확히 맞추기\n        sample_sub = pd.read_csv(cfg.submission_csv)\n        \n        # 컬럼 순서와 내용을 샘플과 정확히 일치\n        submission_df = submission_df.reindex(columns=sample_sub.columns, fill_value=0.0)\n        \n        print(f\"Submission shape: {submission_df.shape}\")\n        print(f\"Sample submission shape: {sample_sub.shape}\")\n        print(f\"Columns match: {list(sample_sub.columns) == list(submission_df.columns)}\")\n        \n        return submission_df\n        \n    except Exception as e:\n        print(f\"Error creating submission: {e}\")\n        import traceback\n        traceback.print_exc()\n        # 빈 제출 파일 생성\n        sample_sub = pd.read_csv(cfg.submission_csv)\n        return sample_sub.fillna(0.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    start_time = time.time()\n    print(\"Starting BirdCLEF-2025 inference...\")\n    \n    # 모델 로딩\n    models = load_models(cfg, num_classes)\n    \n    if not models or 'feature_extractor' not in models:\n        print(\"Critical: Models not loaded properly!\")\n        return\n    \n    if not models.get('lgbm_models'):\n        print(\"Critical: LightGBM models not loaded!\")\n        return\n    \n    engine_info = \"PyTorch\"\n    tta_info = f\"TTA enabled ({cfg.tta_count} augmentations)\" if cfg.use_tta else \"TTA disabled\"\n    valid_lgbm_count = len([m for m in models['lgbm_models'] if m is not None])\n    print(f\"Model usage: {engine_info} ConvNeXt + {valid_lgbm_count}/{len(models['lgbm_models'])} LightGBM models with {tta_info}\")\n\n    # 추론 실행\n    row_ids, predictions = run_inference(cfg, models, species_ids)\n    \n    if not row_ids or not predictions:\n        print(\"No predictions generated!\")\n        return\n\n    # 제출 파일 생성\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n\n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n    \n    end_time = time.time()\n    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}