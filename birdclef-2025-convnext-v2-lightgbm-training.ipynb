{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":11918002,"sourceType":"datasetVersion","datasetId":7492364},{"sourceId":243883542,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport logging\nimport random\nimport gc\nimport time\nimport cv2\nimport math\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport librosa\nimport multiprocessing\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nfrom torchvision.ops import sigmoid_focal_loss\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport timm\nimport lightgbm as lgb\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        return sigmoid_focal_loss(\n            inputs, targets,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            reduction=self.reduction\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # Basic\n    seed = 42\n    debug = False\n    apex = False\n    print_freq = 100\n    num_workers = max(1, int(multiprocessing.cpu_count() * 0.8))\n\n    # Paths\n    OUTPUT_DIR = '/kaggle/working/'\n    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    fabio_csv_path = '/kaggle/input/fabio-csv/fabio.csv'\n    batch_spectrograms_dir = '/kaggle/input/birdclef2025-melspecs-256x256-5sec-16bit'\n\n    # model\n    in_channels = 1\n    num_classes = None  # Will be set dynamically\n\n    # data\n    FS = 32000\n    TARGET_DURATION = 5.0\n    TARGET_SHAPE = (256, 256)\n    \n    # Audio\n    N_FFT = 1024\n    HOP_LENGTH = 512\n    N_MELS = 128\n    FMIN = 50\n    FMAX = 14000\n    \n    # Training\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    epochs = 10\n    batch_size = 32\n    criterion = 'FocalBCE'\n\n    # Augmentation\n    aug_prob = 1.0\n    mixup_prob = 0.6     # 1? 0.8? 0.6?\n    mixup_alpha = 0.4\n\n    def update_debug_settings(self):\n        if self.debug:\n            self.epochs = 2\n\ncfg = CFG()\nprint(cfg.num_workers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(cfg.seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BirdCLEFDatasetFromNPY(Dataset):\n    def __init__(self, df, cfg, spectrograms=None, negative_spectrograms=None, mode=\"train\"):\n        self.df = df.copy()\n        self.cfg = cfg\n        self.mode = mode\n        self.spectrograms = spectrograms\n        self.negative_spectrograms = negative_spectrograms\n\n        # Load Fabio intervals\n        if os.path.exists(cfg.fabio_csv_path):\n            fabio_df = pd.read_csv(cfg.fabio_csv_path)\n            self.fabio_intervals = {row['filename']: (row['start'], row['stop']) for _, row in fabio_df.iterrows()}\n        else:\n            self.fabio_intervals = {}\n        \n        # Load taxonomy (한 번만)\n        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n        self.species_ids = taxonomy_df['primary_label'].tolist()\n        self.num_classes = len(self.species_ids)\n        self.label_to_idx = {label: idx for idx, label in enumerate(self.species_ids)}\n        self.valid_species = set(self.species_ids)  # 미리 set으로 변환\n        \n        # Update config\n        cfg.num_classes = self.num_classes\n\n        # File paths\n        if 'filepath' not in self.df.columns:\n            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df['filename']\n\n        # Sample name: key of spectrogram dictionary\n        if 'samplename' not in self.df.columns:\n            self.df['samplename'] = self.df['filename'].map(\n                lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0]\n            )\n\n        # Debug mode\n        if cfg.debug:\n            print(f\"Debug mode: sampling {min(1000, len(self.df))} samples before expansion\")\n            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n\n        # 모든 스펙트로그램 세그먼트를 개별 샘플로 확장 (최적화된 버전)\n        if spectrograms and mode == \"train\":\n            print(\"Building global spectrogram index...\")\n            self.global_spec_index = self._build_global_index(spectrograms)\n            \n            print(\"Expanding dataset with optimized processing...\")\n            self.df = self._expand_dataframe_ultimate_optimized(self.df)\n            print(f\"Expanded to {len(self.df)} samples with all segments\")\n\n        # Add negative samples to dataframe if provided\n        if self.negative_spectrograms is not None and mode == \"train\":\n            negative_df = pd.DataFrame({\n                'filename': list(self.negative_spectrograms.keys()),\n                'primary_label': ['nocall'] * len(self.negative_spectrograms),\n                'samplename': list(self.negative_spectrograms.keys()),\n                'filepath': [''] * len(self.negative_spectrograms)\n            })\n            self.df = pd.concat([self.df, negative_df], ignore_index=True)\n\n        # 클래스별 샘플 수 계산\n        if mode == \"train\":\n            self.class_counts = self.df['primary_label'].value_counts().to_dict()\n            self.rare_threshold = 20\n            self.target_samples = 50\n            print(f\"Classes with < {self.rare_threshold} samples: {sum(1 for count in self.class_counts.values() if count < self.rare_threshold)}\")\n            \n        # Check spectrograms availability\n        if self.spectrograms:\n            sample_names = set(self.df['samplename'])\n            found_samples = sum(1 for name in sample_names if name in self.spectrograms)\n            print(f\"Found {found_samples} matching positive spectrograms for {mode} dataset\")\n        \n        if self.negative_spectrograms:\n            neg_sample_names = set(self.negative_spectrograms.keys())\n            found_neg_samples = len(neg_sample_names)\n            print(f\"Found {found_neg_samples} negative spectrograms for {mode} dataset\")\n\n    def _build_global_index(self, spectrograms):\n        \"\"\"전역 스펙트로그램 인덱스 한 번만 구축 - 최적화\"\"\"\n        from collections import defaultdict\n        \n        global_index = defaultdict(list)\n        \n        # 벡터화된 키 처리\n        positive_keys = [k for k in spectrograms.keys() if not k.startswith('negative-')]\n        \n        for key in positive_keys:\n            # species 필터링\n            species_id = key.split('-', 1)[0]\n            if species_id in self.valid_species:\n                base_key = key.rsplit('_', 1)[0]\n                global_index[base_key].append(key)\n        \n        print(f\"Built global index with {len(global_index)} base keys from {len(positive_keys)} total keys\")\n        return global_index\n\n    def _expand_dataframe_ultimate_optimized(self, df):\n        \"\"\"메모리 효율성을 높인 궁극의 최적화 버전\"\"\"\n        \n        # 1. 조기 필터링 - 매칭 가능한 샘플만 추출\n        available_samples = set(self.global_spec_index.keys())\n        df_filtered = df[df['samplename'].isin(available_samples)].copy()\n        \n        print(f\"Pre-filtered: {len(df)} → {len(df_filtered)} samples\")\n        \n        if len(df_filtered) == 0:\n            print(\"Warning: No matching samples found!\")\n            return pd.DataFrame()\n        \n        # 2. 청크 단위 처리로 메모리 효율성 향상\n        chunk_size = 1000\n        expanded_chunks = []\n        df_records = df_filtered.to_dict('records')\n        \n        for i in range(0, len(df_records), chunk_size):\n            chunk = df_records[i:i+chunk_size]\n            chunk_expanded = []\n            \n            for row_dict in chunk:\n                samplename = row_dict['samplename']\n                segment_keys = self.global_spec_index.get(samplename, [])\n                \n                # 각 세그먼트에 대해 새 행 생성\n                for segment_key in segment_keys:\n                    new_row = row_dict.copy()\n                    new_row['samplename'] = segment_key\n                    new_row['segment_id'] = segment_key.split('_')[-1]\n                    chunk_expanded.append(new_row)\n            \n            if chunk_expanded:\n                expanded_chunks.append(pd.DataFrame(chunk_expanded))\n            \n            # 메모리 정리\n            del chunk_expanded\n            gc.collect()\n        \n        if expanded_chunks:\n            result_df = pd.concat(expanded_chunks, ignore_index=True)\n            print(f\"Generated {len(result_df)} expanded samples\")\n            return result_df\n        else:\n            return pd.DataFrame()\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # 입력 데이터 검증\n        if idx >= len(self.df):\n            print(f\"Warning: Index {idx} out of range for dataset of size {len(self.df)}\")\n            return None\n            \n        row = self.df.iloc[idx]\n        \n        # 필수 컬럼 존재 확인\n        required_columns = ['samplename', 'filename', 'primary_label']\n        for col in required_columns:\n            if col not in row or pd.isna(row[col]):\n                print(f\"Warning: Missing {col} for index {idx}\")\n                return None\n                \n        samplename = row['samplename']\n        filename = row['filename']\n        primary_label = row['primary_label']\n        \n        # Get spectrogram with error handling\n        spec = self._get_spectrogram(samplename)\n        \n        if spec is None:\n            spec = np.zeros(self.cfg.TARGET_SHAPE, dtype=np.float32)\n            if self.mode == \"train\":\n                print(f\"Warning: No spectrogram found for {samplename}. Using zero spectrogram.\")\n\n        # uint16 → float32\n        if hasattr(spec, 'dtype') and spec.dtype == np.uint16:\n            spec = spec.astype(np.float32) / 65535.0\n\n        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n\n        # Apply augmentations\n        if self.mode == \"train\" and random.random() < self.cfg.aug_prob:\n            spec = self.apply_spec_augmentations(spec, primary_label)\n            \n        # Handle secondary labels\n        secondary_labels = []\n        if 'secondary_labels' in row and pd.notna(row['secondary_labels']) and row['secondary_labels'] != '':\n            secondary_labels = self._parse_secondary_labels(row['secondary_labels'])\n            \n        # Encode labels\n        target = self.encode_label(primary_label, secondary_labels)\n        \n        # Apply mixup (60% 확률로 고정)\n        if self.mode == \"train\" and random.random() < 0.6:\n            spec, target = self._apply_mixup(spec, target, idx)\n        \n        return {\n            'melspec': spec, \n            'target': torch.tensor(target, dtype=torch.float32),\n            'filename': filename\n        }\n    \n    def _get_spectrogram(self, samplename):\n        \"\"\"Get spectrogram from cache with error handling\"\"\"\n        try:\n            if self.spectrograms and samplename in self.spectrograms:\n                spec = self.spectrograms[samplename]\n                # 스펙트로그램 유효성 검사\n                if spec is None or spec.size == 0:\n                    print(f\"Warning: Empty spectrogram for {samplename}\")\n                    return None\n                return spec\n            elif self.negative_spectrograms and samplename in self.negative_spectrograms:\n                spec = self.negative_spectrograms[samplename]\n                if spec is None or spec.size == 0:\n                    print(f\"Warning: Empty negative spectrogram for {samplename}\")\n                    return None\n                return spec\n            else:\n                return None\n        except Exception as e:\n            print(f\"Error loading spectrogram for {samplename}: {e}\")\n            return None\n    \n    def _parse_secondary_labels(self, secondary_labels):\n        \"\"\"Parse secondary labels from string or list\"\"\"\n        if isinstance(secondary_labels, str):\n            try:\n                return eval(secondary_labels)\n            except:\n                return []\n        elif isinstance(secondary_labels, list):\n            return secondary_labels\n        return []\n\n    def _smart_mixup_pairing(self, idx, target):\n        \"\"\"동일하거나 유사한 클래스끼리 우선 페어링 - 최적화\"\"\"\n        current_classes = np.where(target > 0)[0]\n        \n        # 캐시된 positive indices 사용 (성능 향상)\n        if not hasattr(self, '_positive_indices'):\n            self._positive_indices = [i for i, row in self.df.iterrows() if row['primary_label'] != 'nocall']\n        \n        positive_candidates = [i for i in self._positive_indices if i != idx]\n        same_class_candidates = []\n        \n        # 샘플링으로 후보 수 제한 (성능 향상)\n        if len(positive_candidates) > 1000:\n            positive_candidates = random.sample(positive_candidates, 1000)\n        \n        for i in positive_candidates:\n            candidate_target = self.encode_label(self.df.iloc[i]['primary_label'])\n            candidate_classes = np.where(candidate_target > 0)[0]\n            \n            if len(np.intersect1d(current_classes, candidate_classes)) > 0:\n                same_class_candidates.append(i)\n        \n        if same_class_candidates:\n            return random.choice(same_class_candidates)\n        elif positive_candidates:\n            return random.choice(positive_candidates)\n        else:\n            return random.randint(0, len(self.df) - 1)\n       \n    def _apply_mixup(self, spec, target, idx):\n        row = self.df.iloc[idx]\n        \n        # Dismiss negative \n        if row['primary_label'] == 'nocall':\n            return spec, target\n        \n        # pairing\n        mix_idx = self._smart_mixup_pairing(idx, target)\n        row2 = self.df.iloc[mix_idx]\n        \n        # Load spectrogram\n        spec2 = self._get_spectrogram(row2['samplename'])\n        if spec2 is None:\n            return spec, target\n        \n        # uint16 → float32 변환 (필요시)\n        if hasattr(spec2, 'dtype') and spec2.dtype == np.uint16:\n            spec2 = spec2.astype(np.float32) / 65535.0\n            \n        spec2 = torch.tensor(spec2, dtype=torch.float32).unsqueeze(0)\n        \n        # Target encoding\n        target2 = self.encode_label(row2['primary_label'])\n        \n        # Deal with Secondary labels\n        if 'secondary_labels' in row2 and pd.notna(row2['secondary_labels']) and row2['secondary_labels'] != '':\n            secondary_labels2 = self._parse_secondary_labels(row2['secondary_labels'])\n            for label in secondary_labels2:\n                if label in self.label_to_idx:\n                    target2[self.label_to_idx[label]] = 1.0\n        \n        # Set alpha\n        alpha = random.uniform(0.2, 0.8)\n        lam = np.random.beta(alpha, alpha)\n        \n        # Mixup\n        mixed_spec = lam * spec + (1 - lam) * spec2\n        mixed_target = lam * target + (1 - lam) * target2\n        \n        return mixed_spec, mixed_target\n    \n    def apply_spec_augmentations(self, spec, primary_label=None):\n        \"\"\"클래스별 적응적 augmentation with improved quality checks\"\"\"\n        \n        # 클래스별 샘플 수 확인\n        if primary_label and hasattr(self, 'class_counts') and primary_label in self.class_counts:\n            sample_count = self.class_counts[primary_label]\n            is_rare_class = sample_count < self.rare_threshold\n            # 로그 스케일로 augmentation 강도 조절\n            aug_strength = min(1.0, math.log(1000 / max(sample_count, 1)) / math.log(10))\n        else:\n            is_rare_class = False\n            aug_strength = 0.5\n        \n        # 스펙트로그램 품질 체크\n        if torch.mean(spec) < 0.01:  # 너무 조용한 스펙트로그램\n            # 노이즈 추가로 학습 신호 강화\n            noise = torch.randn_like(spec) * 0.1\n            spec = spec + noise\n        \n        # Rare class: 강한 augmentation\n        if is_rare_class:\n            aug_prob = 0.8\n            max_techniques = 4\n        else:\n            aug_prob = 0.5\n            max_techniques = 3\n        \n        applied_count = 0\n        \n        # Time masking\n        if random.random() < aug_prob and applied_count < max_techniques:\n            if is_rare_class:\n                num_masks = random.randint(1, 4)\n                for _ in range(num_masks):\n                    width = random.randint(3, 25)\n                    start = random.randint(0, max(1, spec.shape[2] - width))\n                    spec[0, :, start:start+width] = 0\n            else:\n                num_masks = random.randint(1, 3)\n                for _ in range(num_masks):\n                    width = random.randint(5, 20)\n                    start = random.randint(0, max(1, spec.shape[2] - width))\n                    spec[0, :, start:start+width] = 0\n            applied_count += 1\n        \n        # Frequency masking\n        if random.random() < aug_prob and applied_count < max_techniques:\n            if is_rare_class:\n                num_masks = random.randint(1, 4)\n                for _ in range(num_masks):\n                    height = random.randint(3, 25)\n                    start = random.randint(0, max(1, spec.shape[1] - height))\n                    spec[0, start:start+height, :] = 0\n            else:\n                num_masks = random.randint(1, 3)\n                for _ in range(num_masks):\n                    height = random.randint(5, 20)\n                    start = random.randint(0, max(1, spec.shape[1] - height))\n                    spec[0, start:start+height, :] = 0\n            applied_count += 1\n        \n        # Random brightness/contrast\n        if random.random() < aug_prob and applied_count < max_techniques:\n            if is_rare_class:\n                gain = random.uniform(0.7, 1.3)\n                bias = random.uniform(-0.15, 0.15)\n            else:\n                gain = random.uniform(0.8, 1.2)\n                bias = random.uniform(-0.1, 0.1)\n            \n            spec = spec * gain + bias\n            spec = torch.clamp(spec, 0, 1)\n            applied_count += 1\n\n        # Gaussian noise\n        if random.random() < aug_prob and applied_count < max_techniques:\n            if is_rare_class:\n                noise_level = random.uniform(0.03, 0.08)\n            else:\n                noise_level = 0.05\n            \n            noise = torch.randn_like(spec) * noise_level\n            spec = spec + noise\n            spec = torch.clamp(spec, 0, 1)\n            applied_count += 1\n\n        # Random erasing\n        if random.random() < aug_prob and applied_count < max_techniques:\n            if is_rare_class:\n                num_erases = random.randint(1, 3)\n                for _ in range(num_erases):\n                    erase_height = random.randint(3, 25)\n                    erase_width = random.randint(3, 25)\n                    max_x = spec.shape[2] - erase_width\n                    max_y = spec.shape[1] - erase_height\n                    if max_x > 0 and max_y > 0:\n                        x = random.randint(0, max_x)\n                        y = random.randint(0, max_y)\n                        spec[0, y:y+erase_height, x:x+erase_width] = 0\n            else:\n                erase_height = random.randint(5, 20)\n                erase_width = random.randint(5, 20)\n                max_x = spec.shape[2] - erase_width\n                max_y = spec.shape[1] - erase_height\n                if max_x > 0 and max_y > 0:\n                    x = random.randint(0, max_x)\n                    y = random.randint(0, max_y)\n                    spec[0, y:y+erase_height, x:x+erase_width] = 0\n            applied_count += 1\n            \n        return spec\n    \n    def encode_label(self, label, secondary_labels=None):\n        target = np.zeros(self.num_classes)\n        if label == 'nocall' or label == '':\n            return target\n        if label in self.label_to_idx:\n            target[self.label_to_idx[label]] = 1.0\n        if secondary_labels:\n            for sec in secondary_labels:\n                if sec in self.label_to_idx:\n                    target[self.label_to_idx[sec]] = 1.0\n        return target\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return {}\n        \n    result = {key: [] for key in batch[0].keys()}\n    \n    for item in batch:\n        for key, value in item.items():\n            result[key].append(value)\n    \n    for key in result:\n        if key in ['target', 'melspec'] and isinstance(result[key][0], torch.Tensor):\n            try:\n                result[key] = torch.stack(result[key])\n            except RuntimeError as e:\n                print(f\"Error stacking {key}: {e}\")\n                continue\n    \n    return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvNeXtFeatureExtractor(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        # 개선된 ConvNeXt 모델 설정\n        self.backbone = timm.create_model(\n            'convnextv2_nano.fcmae',\n            pretrained=True,\n            in_chans=cfg.in_channels,\n            num_classes=0,  # Remove classification layer\n            drop_rate=0.2,  # 드롭아웃 추가\n            drop_path_rate=0.1  # 드롭패스 추가\n        )\n        \n        # 더 효과적인 풀링 전략\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.global_pool = nn.AdaptiveMaxPool2d(1)  # 추가\n\n    def forward(self, x):\n        features = self.backbone(x)  # (B, C, H, W)\n        if len(features.shape) == 4:\n            avg_pool = self.pooling(features).view(features.size(0), -1)  # (B, C)\n            max_pool = self.global_pool(features).view(features.size(0), -1)  # (B, C)\n            # 평균과 최대 풀링 결합\n            return torch.cat([avg_pool, max_pool], dim=1)\n        return features\n\ndef extract_features(model, dataloader, device):\n    \"\"\"Extract features using the model with improved memory management\"\"\"\n    model.eval()\n    features = []\n    labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n            if 'melspec' not in batch or 'target' not in batch:\n                continue\n                \n            x = batch['melspec'].to(device)\n            feats = model(x)  # (B, C)\n            features.append(feats.cpu().numpy())\n            labels.append(batch['target'].cpu().numpy())\n            \n            # GPU 메모리 정리 추가\n            del x, feats\n            torch.cuda.empty_cache()\n            \n    if features:\n        features = np.concatenate(features, axis=0)\n        labels = np.concatenate(labels, axis=0)\n        return features, labels\n    else:\n        return np.array([]), np.array([])\n\ndef train_lightgbm(X, y):\n    \"\"\"Train LightGBM models with optimized hyperparameters\"\"\"\n    models = []\n    print(f\"Training LightGBM for {y.shape[1]} classes...\")\n    \n    # 개선된 기본 파라미터 설정\n    base_params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.9,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'verbose': -1,\n        'min_data_in_leaf': 20,  # 추가\n        'lambda_l1': 0.1,        # 추가\n        'lambda_l2': 0.1,        # 추가\n        'max_depth': -1          # 추가\n    }\n    y_binary = (y > 0.5).astype(int)\n    \n    for i in tqdm(range(y_binary.shape[1]), desc=\"Training LightGBM\"):\n        try:\n            # 클래스별 동적 파라미터 조정\n            class_positive_count = np.sum(y_binary[:, i])\n            params = base_params.copy()\n            \n            if class_positive_count < 50:  # 희귀 클래스\n                params['learning_rate'] = 0.03\n                params['num_leaves'] = 15\n                params['min_data_in_leaf'] = 5\n            \n            # 먼저 stratified split 시도\n            X_train, X_val, y_train, y_val = train_test_split(\n                X, y[:, i], \n                test_size=0.2, \n                random_state=42,\n                stratify=y[:, i]\n            )\n        except ValueError as e:\n            # Stratify 실패 시 random split으로 대체\n            print(f\"Class {i}: Stratify failed ({str(e)}), using random split\")\n            X_train, X_val, y_train, y_val = train_test_split(\n                X, y[:, i], \n                test_size=0.2, \n                random_state=42,\n                stratify=None\n            )\n        \n        lgb_train = lgb.Dataset(X_train, label=y_train)\n        lgb_val = lgb.Dataset(X_val, label=y_val)\n        \n        model = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=100,\n            valid_sets=[lgb_val],\n            valid_names=['valid'],\n            callbacks=[\n                lgb.early_stopping(10),\n                lgb.log_evaluation(0)\n            ]\n        )\n        models.append(model)\n    \n    return models\n\n\ndef load_spectrograms_batch(cfg, batch_dir):\n    \"\"\"Load spectrograms from batch npy files\"\"\"\n    import glob\n    \n    # batch npy 파일들 찾기\n    batch_files = glob.glob(os.path.join(batch_dir, '*.npy'))\n    batch_files = [f for f in batch_files if 'melspecs_uint16_batch' in f]\n    batch_files.sort()\n    \n    if not batch_files:\n        print(f\"No batch files found in {batch_dir}\")\n        return None, None\n    \n    print(f\"Loading spectrograms from {len(batch_files)} batch files...\")\n    \n    all_spectrograms = {}\n    for batch_file in batch_files:\n        print(f\"Loading {os.path.basename(batch_file)}...\")\n        batch_data = np.load(batch_file, allow_pickle=True).item()\n        \n        # uint16 → float32 변환\n        for key, spec in batch_data.items():\n            if spec.dtype == np.uint16:\n                spec = spec.astype(np.float32) / 65535.0\n            all_spectrograms[key] = spec\n    \n    # positive, negative 분리\n    positive_spectrograms = {k: v for k, v in all_spectrograms.items() if not k.startswith('negative-')}\n    negative_spectrograms = {k: v for k, v in all_spectrograms.items() if k.startswith('negative-')}\n    \n    print(f\"Loaded {len(positive_spectrograms)} positive spectrograms\")\n    print(f\"Loaded {len(negative_spectrograms)} negative spectrograms\")\n    \n    return positive_spectrograms, negative_spectrograms","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport json\nfrom datetime import datetime\n\ndef main():\n    \"\"\"Main training function\"\"\"\n    print(\"Starting BirdCLEF 2025 training pipeline...\")\n    \n    # Update debug settings\n    cfg.update_debug_settings()\n    \n    # Load data\n    print(\"Loading training data...\")\n    train_df = pd.read_csv(cfg.train_csv)\n    print(f\"Loaded {len(train_df)} training samples\")\n    \n    # Load spectrograms\n    positive_spectrograms, negative_spectrograms = load_spectrograms_batch(cfg, cfg.batch_spectrograms_dir)\n            \n    # Create feature extractor\n    print(\"Initializing ConvNeXt feature extractor...\")\n    feature_model = ConvNeXtFeatureExtractor(cfg).to(cfg.device)\n    \n    # Create dataset and dataloader\n    print(\"Creating dataset...\")\n    train_dataset = BirdCLEFDatasetFromNPY(\n        train_df, \n        cfg, \n        spectrograms=positive_spectrograms, \n        negative_spectrograms=negative_spectrograms,\n        mode='train'\n    )\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=cfg.batch_size, \n        shuffle=False,\n        num_workers=cfg.num_workers,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    \n    # Extract features\n    print(\"Extracting features...\")\n    X_train, y_train = extract_features(feature_model, train_loader, cfg.device)\n    \n    if len(X_train) == 0:\n        print(\"No features extracted. Check your data and model.\")\n        return\n    \n    print(f\"Extracted features shape: {X_train.shape}\")\n    print(f\"Labels shape: {y_train.shape}\")\n    print(\"Positive count per class:\", np.sum(y_train, axis=0))\n\n    # Check positive/negative ratio\n    positive_samples = np.sum(np.any(y_train == 1, axis=1))\n    negative_samples = len(y_train) - positive_samples\n    print(f\"Positive samples: {positive_samples}\")\n    print(f\"Negative samples: {negative_samples}\")\n    print(f\"Negative ratio: {negative_samples/len(y_train)*100:.1f}%\")\n    \n    # Train LightGBM\n    print(\"Training LightGBM models...\")\n    lgbm_models = train_lightgbm(X_train, y_train)\n    \n    print(f\"Training completed! Trained {len(lgbm_models)} LightGBM models.\")\n    \n    # Save models and metadata\n    print(\"Saving models...\")\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    \n    # 1. Save LightGBM models\n    models_dir = os.path.join(cfg.OUTPUT_DIR, f'lgbm_models_{timestamp}')\n    os.makedirs(models_dir, exist_ok=True)\n    \n    for i, model in enumerate(lgbm_models):\n        model_path = os.path.join(models_dir, f'lgbm_model_class_{i}.txt')\n        model.save_model(model_path)\n    \n    print(f\"Saved {len(lgbm_models)} LightGBM models to {models_dir}\")\n    \n    # 2. Save ConvNeXt feature extractor\n    feature_model_path = os.path.join(cfg.OUTPUT_DIR, f'convnext_feature_extractor_{timestamp}.pth')\n    torch.save({\n        'model_state_dict': feature_model.state_dict(),\n        'model_config': {\n            'model_name': 'convnextv2_nano.fcmae',\n            'in_chans': cfg.in_channels,\n            'num_classes': 0\n        }\n    }, feature_model_path)\n    \n    print(f\"Saved ConvNeXt feature extractor to {feature_model_path}\")\n    \n    # 3. Save species mapping and metadata\n    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n    species_mapping = {\n        'species_ids': taxonomy_df['primary_label'].tolist(),\n        'label_to_idx': {label: idx for idx, label in enumerate(taxonomy_df['primary_label'])},\n        'num_classes': len(taxonomy_df)\n    }\n    \n    metadata = {\n        'timestamp': timestamp,\n        'training_samples': len(train_df),\n        'extracted_features_shape': X_train.shape,\n        'labels_shape': y_train.shape,\n        'positive_samples': int(positive_samples),\n        'negative_samples': int(negative_samples),\n        'negative_ratio': float(negative_samples/len(y_train)*100),\n        'num_lgbm_models': len(lgbm_models),\n        'cfg_settings': {\n            'model_name': 'convnextv2_nano.fcmae',\n            'batch_size': cfg.batch_size,\n            'TARGET_SHAPE': cfg.TARGET_SHAPE,\n            'aug_prob': cfg.aug_prob,\n            'mixup_alpha': cfg.mixup_alpha,\n            'debug': cfg.debug\n        },\n        'species_mapping': species_mapping\n    }\n    \n    # Save metadata as JSON\n    metadata_path = os.path.join(cfg.OUTPUT_DIR, f'training_metadata_{timestamp}.json')\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"Saved training metadata to {metadata_path}\")\n    \n    # 4. Save class counts for inference\n    if hasattr(train_dataset, 'class_counts'):\n        class_counts_path = os.path.join(cfg.OUTPUT_DIR, f'class_counts_{timestamp}.json')\n        with open(class_counts_path, 'w') as f:\n            json.dump(train_dataset.class_counts, f, indent=2)\n        print(f\"Saved class counts to {class_counts_path}\")\n    \n    # 5. Create inference config file\n    inference_config = {\n        'models_dir': models_dir,\n        'feature_extractor_path': feature_model_path,\n        'metadata_path': metadata_path,\n        'class_counts_path': class_counts_path if hasattr(train_dataset, 'class_counts') else None,\n        'num_classes': len(taxonomy_df),\n        'target_shape': cfg.TARGET_SHAPE,\n        'model_name': cfg.model_name,\n        'timestamp': timestamp\n    }\n    \n    inference_config_path = os.path.join(cfg.OUTPUT_DIR, f'inference_config_{timestamp}.json')\n    with open(inference_config_path, 'w') as f:\n        json.dump(inference_config, f, indent=2)\n    \n    print(f\"Saved inference config to {inference_config_path}\")\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"TRAINING SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Timestamp: {timestamp}\")\n    print(f\"Training samples: {len(train_df):,}\")\n    print(f\"Features shape: {X_train.shape}\")\n    print(f\"Positive samples: {positive_samples:,}\")\n    print(f\"Negative samples: {negative_samples:,}\")\n    print(f\"LightGBM models: {len(lgbm_models)}\")\n    print(\"\\nSaved files:\")\n    print(f\"- LightGBM models: {models_dir}\")\n    print(f\"- Feature extractor: {feature_model_path}\")\n    print(f\"- Metadata: {metadata_path}\")\n    print(f\"- Inference config: {inference_config_path}\")\n    if hasattr(train_dataset, 'class_counts'):\n        print(f\"- Class counts: {class_counts_path}\")\n    print(\"=\"*50)\n    \n    return {\n        'lgbm_models': lgbm_models,\n        'feature_model': feature_model,\n        'metadata': metadata,\n        'inference_config': inference_config,\n        'timestamp': timestamp\n    }\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}